# -*- coding: utf-8 -*-
"""DataHack.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ATVClEbg-kSg1spSYqp6O0PBi0h1Wm8t
"""

!pip install haystack-ai "transformers[torch,sentencepiece]"
!pip install pdfplumber
!pip install peft
!pip install youtube-transcript-api
!pip install pytube
!pip install spacy

import pdfplumber
with pdfplumber.open("/content/Resume_Soham_Y.pdf") as pdf:
    for page in pdf.pages:
        text += page.extract_text()
        #print(text)

import heapq
from pytube import extract
from heapq import nlargest
from youtube_transcript_api import YouTubeTranscriptApi
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation

url = 'https://youtu.be/cUBz04LlLVk?si=UbT2fsPZIvs_TXha'
video_id = extract.video_id(url)
transcript = YouTubeTranscriptApi.get_transcript(video_id)
text = ""
for elem in transcript:
    text = text + " " + elem["text"]

import spacy
from collections import Counter
import random

# English tokenizer
nlp_model = spacy.load("en_core_web_sm")
def create_mcqs(input_text, question_count=5):
    processed_doc = nlp_model(input_text)
    sentence_list = [sentence.text for sentence in processed_doc.sents]
    selected_sentences = random.sample(sentence_list, min(question_count, len(sentence_list)))
    mcq_list = []
    for sentence in selected_sentences:
        sentence_doc = nlp_model(sentence)
        noun_list = [token.text for token in sentence_doc if token.pos_ == "NOUN"]
        if len(noun_list) < 2:
            continue
        noun_frequency = Counter(noun_list)
        if noun_frequency:
            main_noun = noun_frequency.most_common(1)[0][0]
            question_format = sentence.replace(main_noun, "__________")
            choices = [main_noun]
            for _ in range(3):
                distractor = random.choice(list(set(noun_list) - set([main_noun])))
                choices.append(distractor)

            random.shuffle(choices)

            correct_option = chr(64 + choices.index(main_noun) + 1)
            mcq_list.append((question_format, choices, correct_option))

    return mcq_list

mcqs_tech = create_mcqs(text)

# Prin generat MCQ
for i, mcq in enumerate(mcqs_tech, start=1):
    question_stem, answer_choices, correct_answer = mcq
    print(f"Q{i}: {question_stem}?")
    for j, choice in enumerate(answer_choices, start=1):
        print(f"{chr(64+j)}: {choice}")
    print(f"Correct Answer: {correct_answer}\n")

from haystack import Document
from haystack.components.readers import ExtractiveReader

docs = [
    #Document(content=text),
    Document(content="Soham is tom's step-Dad"),
    Document(content="mom and dad of anyone are husband and wife to each other"),
    Document(content="lala is toms's mom")
]

reader = ExtractiveReader(model="deepset/roberta-base-squad2")
reader.warm_up()

question = "who is john"

result = reader.run(query=question, documents=docs)
print(result)

# Load model directly
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("naver-clova-ix/donut-base-finetuned-docvqa")
model = AutoModel.from_pretrained("naver-clova-ix/donut-base-finetuned-docvqa")

model(
    "https://templates.invoicehome.com/invoice-template-us-neat-750px.png",
    "What is the invoice number?"
)
# {'score': 0.9943977, 'answer': 'us-001', 'start': 15, 'end': 15}

"""nlp(
    "https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg",
    "What is the purchase amount?"
)
# {'score': 0.9912159, 'answer': '$1,000,000,000', 'start': 97, 'end': 97}

nlp(
    "https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png",
    "What are the 2020 net sales?"
)
# {'score': 0.59147286, 'answer': '$ 3,750', 'start': 19, 'end': 20}"""

from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration
import requests
from PIL import Image

processor = Pix2StructProcessor.from_pretrained('google/deplot')
model = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')

url = "https://preview.redd.it/gdp-per-capita-of-g20-v0-qdkgzqvv7mlb1.png?auto=webp&s=8186c3a76bf4fd298e2e1a62138093677fa461f8"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, text="Generate underlying data table of the figure below:", return_tensors="pt")
predictions = model.generate(**inputs, max_new_tokens=512)
print(processor.decode(predictions[0], skip_special_tokens=True))

!pip install wikipedia-api

import wikipediaapi

# Define a user agent string that describes your bot
user_agent = "MyWikipediaBot/1.0 (sohamyedgaonkar@gmail.com)"

# Pass the user agent to the Wikipedia class constructor
wiki_wiki = wikipediaapi.Wikipedia(
    language='en',
    user_agent=user_agent # Specify the user agent here
)

page = wiki_wiki.page("Python programming")

if page.exists():
    print(f"Title: {page.title}")
    print(page.text)
else:
    print("Page does not exist.")

import heapq   # Module will be used to generate a summary from tockenized sentences
from pytube import extract # module will be used to interact with youtube Youtube using Video URL
from heapq import nlargest
from youtube_transcript_api import YouTubeTranscriptApi # module will be used to get trasncaript of video
import spacy # module will be used  to build NLP model
from spacy.lang.en.stop_words import STOP_WORDS # module will be used  to build NLP model
from string import punctuation

url = 'https://youtu.be/cUBz04LlLVk?si=UbT2fsPZIvs_TXha'
video_id = extract.video_id(url)
transcript = YouTubeTranscriptApi.get_transcript(video_id)
text = ""
for elem in transcript:
    text = text + " " + elem["text"]
text

-----------------------------------------------------------------for mcq generation-----------------------------------------------------------------

